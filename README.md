# Proyecto ETL en Python

Este repositorio contiene un **pipeline ETL (Extract, Transform, Load)** desarrollado en Python. El objetivo del proyecto es **procesar archivos de Clientes y Tarjetas**, aplicar **validaciones, limpieza y normalizaciÃ³n de datos**, registrar errores y finalmente **cargar la informaciÃ³n vÃ¡lida en una base de datos**.

El proyecto estÃ¡ pensado con una **arquitectura modular**, separando responsabilidades (lectura, validaciÃ³n, limpieza, carga, logging), lo que facilita el mantenimiento, la escalabilidad y las pruebas.

---

## ğŸ¯ Objetivo del proyecto

* Automatizar la ingesta de datos desde archivos CSV
* Detectar y registrar errores de calidad de datos
* Generar datasets limpios y consistentes
* Cargar la informaciÃ³n procesada en una base de datos relacional
* Contar con trazabilidad completa del proceso mediante logs

Este proyecto puede utilizarse como **ejercicio acadÃ©mico**, **prueba tÃ©cnica** o **pieza de portafolio para perfiles Data Engineer / Data Analyst**.

---

## ğŸ§± Arquitectura ETL

El pipeline sigue el flujo clÃ¡sico:

1. **Extract**
   Lectura de archivos CSV desde el directorio `data/raw` mediante descubrimiento automÃ¡tico.

2. **Transform**

   * ValidaciÃ³n de reglas de negocio
   * Limpieza de campos
   * NormalizaciÃ³n de formatos
   * SeparaciÃ³n de registros vÃ¡lidos e invÃ¡lidos

3. **Load**

   * Escritura de archivos limpios en `data/output`
   * Registro de errores en `errors/`
   * InserciÃ³n en base de datos usando SQL

---

## ğŸ“‚ Estructura del proyecto (detalle completo)

```
etl-proyecto/
â”œâ”€â”€ config.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ output/
â”œâ”€â”€ errors/
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ reader.py
â”‚   â”œâ”€â”€ file_discovery.py
â”‚   â”œâ”€â”€ validate_clientes.py
â”‚   â”œâ”€â”€ validate_tarjetas.py
â”‚   â”œâ”€â”€ clean_clientes.py
â”‚   â”œâ”€â”€ clean_tarjetas.py
â”‚   â”œâ”€â”€ db_loader.py
â”‚   â”œâ”€â”€ errors.py
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ etl.log
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_pipeline.py
â””â”€â”€ sql/
    â””â”€â”€ schema.sql
```

---

## ğŸ—‚ï¸ DescripciÃ³n de archivos y mÃ³dulos

### ğŸ”§ ConfiguraciÃ³n

#### `config.yaml`

Archivo central de configuraciÃ³n del pipeline. Permite definir:

* Rutas de entrada y salida de datos
* ParÃ¡metros de validaciÃ³n
* ConfiguraciÃ³n de conexiÃ³n a base de datos

Separar la configuraciÃ³n del cÃ³digo permite modificar el comportamiento del ETL sin tocar la lÃ³gica.

---

#### `requirements.txt`

Lista de dependencias necesarias para ejecutar el proyecto. Facilita la instalaciÃ³n del entorno y la reproducibilidad.

---

## ğŸ“ Directorio `data/`

### `data/raw/`

Contiene los **archivos CSV originales** que ingresan al pipeline. Estos archivos no se modifican.

### `data/output/`

Almacena los **archivos procesados y limpios**, listos para anÃ¡lisis o carga en base de datos.

---

## âŒ Directorio `errors/`

AquÃ­ se guardan los registros que **no cumplen las reglas de validaciÃ³n**. Cada archivo contiene:

* Fila original
* Motivo del rechazo

Esto permite auditar errores y mejorar la calidad de los datos de origen.

---

## ğŸ§  Directorio `etl/` (lÃ³gica del pipeline)

### `file_discovery.py`

Se encarga de **detectar automÃ¡ticamente los archivos de entrada** en `data/raw`, evitando hardcodear nombres de archivos.

---

### `reader.py`

Responsable de la **lectura de archivos CSV** y su conversiÃ³n a estructuras manejables (por ejemplo, DataFrames).

---

### `validate_clientes.py`

Contiene las **reglas de validaciÃ³n para datos de clientes**, como:

* Campos obligatorios
* Formatos vÃ¡lidos
* Consistencia de datos

---

### `validate_tarjetas.py`

Define las **validaciones especÃ­ficas para tarjetas**, como:

* Longitud y formato de nÃºmero de tarjeta
* Fechas vÃ¡lidas
* AsociaciÃ³n con clientes existentes

---

### `clean_clientes.py`

Aplica procesos de **limpieza y normalizaciÃ³n** a los datos de clientes:

* EstandarizaciÃ³n de texto
* Manejo de valores nulos
* CorrecciÃ³n de formatos

---

### `clean_tarjetas.py`

Limpieza y transformaciÃ³n de los datos de tarjetas para dejarlos listos para persistencia.

---

### `errors.py`

Centraliza la **gestiÃ³n de errores y registros rechazados**, asegurando consistencia en los archivos generados.

---

### `db_loader.py`

Encargado de la **carga de datos en la base de datos**, utilizando scripts SQL y controlando errores de inserciÃ³n.

---

### `logger.py`

Configura el **sistema de logging**, permitiendo registrar eventos, advertencias y errores durante la ejecuciÃ³n del ETL.

---

## ğŸ“œ Directorio `scripts/`

### `run_pipeline.py`

Script principal del proyecto. Orquesta todo el flujo ETL:

1. Descubrimiento de archivos
2. Lectura
3. ValidaciÃ³n
4. Limpieza
5. Escritura de resultados
6. Carga a base de datos
7. Logging del proceso completo

---

## ğŸ—„ï¸ Directorio `sql/`

### `schema.sql`

Define el **esquema de la base de datos**, incluyendo tablas, tipos de datos y relaciones necesarias para almacenar clientes y tarjetas.

---

## âš™ï¸ Requisitos del sistema

* Python 3.10 o superior
* pip

InstalaciÃ³n de dependencias:

```bash
pip install -r requirements.txt
```

---

## ğŸš€ EjecuciÃ³n del pipeline

Desde la raÃ­z del proyecto:

```bash
python scripts/run_pipeline.py
```

Durante la ejecuciÃ³n:

* Se generan logs en tiempo real
* Los errores se almacenan en archivos separados
* Los datos vÃ¡lidos se escriben en `data/output`

---

## ğŸ“Š Logs

El archivo `logs/etl.log` contiene:

* Inicio y fin del proceso
* Archivos procesados
* Cantidad de registros vÃ¡lidos e invÃ¡lidos
* Detalles de errores

---

## ğŸ”® Posibles mejoras futuras

* Tests automatizados
* Validaciones con esquemas (Great Expectations / Pandera)
* OrquestaciÃ³n con Airflow
* DockerizaciÃ³n del proyecto
* IntegraciÃ³n con APIs externas

---

## ğŸ“„ Licencia

Proyecto de uso educativo y demostrativo. Puede adaptarse y reutilizarse libremente.

---

âœï¸ Desarrollado en Python como proyecto ETL modular y escalable.
